# Explainable Deep Learning – Grad-CAM on CIFAR-10

This project applies **Grad-CAM**, **Grad-CAM++**, and **Score-CAM** to a pretrained **ResNet-50** model to visualize how deep learning models interpret images from the **CIFAR-10** dataset. The goal is to understand which regions the model focuses on when making predictions for different forms of **transportation and movement**, including airplanes, automobiles, ships, trucks, and horses.

## Overview
The experiment investigates model explainability on low-resolution images, exploring how a network trained on high-quality ImageNet data performs under constrained visual conditions. By analyzing Grad-CAM and its variants's heatmaps, the project highlights how attention varies across classes and methods, even when predictions are imperfect.

## Dataset
**CIFAR-10** contains 60,000 color images (32×32) across 10 classes.  
Classes used: `airplane`, `automobile`, `ship`, `truck`, `horse`.  
The images used simulate real-world conditions such as blurry or compressed footage, making it ideal for studying model interpretability under domain shift.

**Citation:**  
Krizhevsky, A. (2009). *Learning Multiple Layers of Features from Tiny Images (CIFAR-10).* University of Toronto.  
[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)

## Model
A pretrained **ResNet-50** from Torchvision (trained on ImageNet-1K) was used without fine-tuning.  
Images were normalized using ImageNet statistics to match the model’s expected input:
**Citation:**  
He, K., Zhang, X., Ren, S., & Sun, J. (2016). *Deep Residual Learning for Image Recognition.* CVPR.  
[https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html](https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet50.html)

## Results Summary
- **Airplane:** Misclassified as “can opener,” but CAMs correctly focused on the airplane body.  
- **Automobile:** Classified as “moving van,” with strong attention on the wheels.  
- **Ship:** Correctly identified, with Grad-CAM highlighting the base and Score-CAM the full body.  
- **Truck:** Grad-CAM focused on headlights and doors; Score-CAM on the opposite side.  
- **Horse:** Grad-CAM variants emphasized legs; Score-CAM captured the full body.

Despite misclassifications, all methods consistently highlighted semantically meaningful regions, showing that explainability tools can still reveal logical model focus areas.

## Key Insight
Grad-CAM visualizations confirmed that model reasoning can remain stable even when predictions are incorrect. This demonstrates the importance of explainability in domains such as **autonomous driving**, **mobility perception**, and **computer vision safety**.

## How to Run
1. Open the notebook in Google Colab.  
2. Install dependencies:  
3. Run all cells in order.  
4. Visualizations for the five selected images will appear automatically.

## References
- Krizhevsky, A. (2009). *Learning Multiple Layers of Features from Tiny Images (CIFAR-10).* University of Toronto.  
- He, K., Zhang, X., Ren, S., & Sun, J. (2016). *Deep Residual Learning for Image Recognition.* CVPR.  
- Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). *Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.* ICCV.

**Author:** Tiffany A. Degbotse  
**Course:** AIPI 590 – Explainable Deep Learning  
**Institution:** Duke University, Pratt School of Engineering  
**Created in:** Google Colab  
**Repository:** [https://github.com/TiffanyDegbotse/Explainable-Deep-Learning](https://github.com/TiffanyDegbotse/Explainable-Deep-Learning)
**This ReadMe was generated by Chatgpt on 10/05/2025 at 8:20pm**
